{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6730fecf",
   "metadata": {},
   "source": [
    "## Assignment 2 \n",
    "### Jiaxuan  a1876484"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df2aac6",
   "metadata": {},
   "source": [
    "### 1. Reading dataset and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e967e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text input for both dataset articles and queries.\n",
    "\n",
    "    text: Text in string.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer() # define lemmatizer\n",
    "    stop_words = set(stopwords.words('english')) # use stop_words\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t.lower() for t in tokens if t.lower() not in stop_words] # eliminate the stopwords\n",
    "    tokens = [t for t in tokens if t not in string.punctuation] # eliminate the stopwords\n",
    "    tokens = [t for t in tokens if not t.isnumeric()] # eliminate the single numbers to clean the text, keep the words like COVID-19.\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens] # lemmatize rest words\n",
    "    return \" \".join(tokens) # return a string after preprocess\n",
    "\n",
    "\n",
    "def read_and_process_data(file_path, num_samples = 1000):\n",
    "    \"\"\"\n",
    "    Randomly load and process the dataset and save in a dataframe in the end.\n",
    "\n",
    "    file_path: Path to the folder containing the JSON files\n",
    "    num_samples: Number of samples to read from the folder (default: 10000)\n",
    "    \"\"\"\n",
    "    files = os.listdir(file_path) # get all the file names in the folder\n",
    "    sample = random.sample(files, num_samples) # randomly pick files with the number of samples\n",
    "    df_sample = pd.DataFrame(columns=['paper_id', 'paragraphs'])# create a dataframe to save the aritcle id and paragraphs \n",
    "    for file in sample: # read files, extract paper_id and texts in bodytext\n",
    "        with open(file_path + file ,encoding = 'utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        paperid = data['paper_id']\n",
    "        bodytext = ''\n",
    "        for text in data['body_text']:\n",
    "            text = text['text']\n",
    "            bodytext += text\n",
    "        df_sample.loc[len(df_sample.index)] = [paperid , bodytext] # add a line with paper_id and all the bodytext in it.\n",
    "    df_sample[\"paragraphs\"] = df_sample[\"paragraphs\"].apply(preprocess_text)# preprocess all the paragraphs.\n",
    "    return df_sample\n",
    "# Here we decided to use a dataframe to save the text as sentences, because we will use word2vec to transform them into word embeddings, \n",
    "# so the \"distance\" between them is very helpful and necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d75afc",
   "metadata": {},
   "source": [
    "### 2. Named Entity Recognition and Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2d286790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_knowledge_base(articles):\n",
    "    \"\"\"\n",
    "    Use NER to create KB, extract all the NEs that are linked by 'also named', 'also called' and 'also known as'\n",
    "\n",
    "    articles: Articles after pre-process\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_sci_sm\") # more accurate extraction of NER using spcay's medical data\n",
    "    kb = {}\n",
    "    def add_to_kb(key: str, value: str): # define adding the detected words to the dictionary\n",
    "        if key not in kb:\n",
    "            kb[key] = [value]\n",
    "        elif value not in kb[key]:\n",
    "            kb[key].append(value)\n",
    "\n",
    "    for _, paragraphs in articles:\n",
    "        doc = nlp(paragraphs)\n",
    "        for token in doc:\n",
    "            # check for aliases using \"also/sometines known as\" pattern\n",
    "            if token.lower_ in {\"also\", \"sometimes\"}:\n",
    "                conj = token.head\n",
    "                if conj.lower_ in {\"known\",\"called\",\"named\"}:\n",
    "                    subject = conj.head\n",
    "                    obj = [t for t in conj.rights if t.dep_ in {\"dobj\", \"attr\"}]\n",
    "                    # add the word after lemmatizer and lower\n",
    "                    add_to_kb(subject.lemma_.lower(), obj[0].lemma_.lower()) \n",
    "\n",
    "            # check for aliases using \"or\" or \"cc\"pattern\n",
    "            if token.lower_ == \"or\" and token.dep_ == \"cc\":\n",
    "                left = token.lefts\n",
    "                right = token.rights\n",
    "                for left_item in left:\n",
    "                    for right_item in right:\n",
    "                        # determine whether the left and right lexicals are consistent with pos_tag\n",
    "                        if (left_item.dep_ in {\"conj\", \"appos\"} and\n",
    "                            right_item.dep_ in {\"conj\", \"appos\"} and\n",
    "                            left_item.pos_ in {\"NOUN\", \"PROPN\"} and\n",
    "                            right_item.pos_ in {\"NOUN\", \"PROPN\"}):\n",
    "                            # add the word after lemmatizer and lower\n",
    "                            add_to_kb(left_item.lemma_.lower(), right_item.lemma_.lower())\n",
    "\n",
    "    return kb\n",
    "\n",
    "#Here I used the simpler method of finding conjunctions to find aliases, \n",
    "#but to be more precise I added a lexical determination so that only nouns would be added to the KB dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333cefa8",
   "metadata": {},
   "source": [
    "### 3. Indexing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e5d6ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Article embeddings\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \"\"\"\n",
    "    Calculate the average word vectors for a list of words.\n",
    "\n",
    "    words: The input list of words\n",
    "    model: Word2Vec model trained on the paragraphs\n",
    "    vocabulary: Set of words in the model's vocabulary\n",
    "    num_features: Size of the word embeddings same to the model size\n",
    "    \"\"\"\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float32\") # initialize an empty feature vector with the same size as the word embeddings\n",
    "    n_words = 0 # initialize a counter for the number of words in the list that are in the model's vocabulary\n",
    "    for word in words: # iterate through the words in the list\n",
    "         \n",
    "        if word in vocabulary: # if the word is in the model's vocabulary, update the feature vector and counter\n",
    "            n_words += 1\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "            \n",
    "    # if there are any valid words in the list, divide the feature vector by the number of valid words\n",
    "    if n_words:\n",
    "        feature_vector = np.divide(feature_vector, n_words)\n",
    "    return feature_vector\n",
    "\n",
    "def create_article_embeddings(processed_data, embedding_size=200):\n",
    "    \"\"\"\n",
    "    Create article embeddings using Word2Vec.\n",
    "\n",
    "    processed_data: DataFrame containing the processed articles\n",
    "    embedding_size: Size of the word embeddings (default: 200)\n",
    "    \"\"\"\n",
    "    # train the Word2Vec model\n",
    "    #model = Word2Vec([paragraph.split() for paragraph in processed_data[\"paragraphs\"]], vector_size=embedding_size, min_count=3,window=5,workers=4)\n",
    "    model = Word2Vec.load(\"word2vec.model\")\n",
    "    # compute the average embeddings for each article\n",
    "    article_embeddings = [average_word_vectors(article.split(), model, set(model.wv.index_to_key), 200) for article in processed_data[\"paragraphs\"]]\n",
    "    #model.save(\"word2vec.model\")\n",
    "    return model, article_embeddings\n",
    "# Here I use Inverted Index to implement article indexing, in average_word_vectors() we define the implementation of converting text into word vectors, \n",
    "# then generate the model and define the word vector output of all articles separately in create_article_embeddings() to compare later with word vecters of query words, \n",
    "# And I use the content of 20,000 files as the training content of my model, the size of the model is about 650MB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067cebcf",
   "metadata": {},
   "source": [
    "### 4. Text matching utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c292820d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4. Text matching utility\n",
    "\n",
    "    \n",
    "def match_articles(query, model,processed_data, article_embeddings, kb, top_k=10):\n",
    "    \"\"\"\n",
    "    Match articles based on the given query.\n",
    "\n",
    "    query: The input query (question)\n",
    "    model: Word2Vec model trained on the paragraphs\n",
    "    processed_data: The dataframe of dataset to find the cord_uid\n",
    "    article_embeddings: Precomputed article embeddings\n",
    "    kb: Knowledge Base containing named entities and their associated terms\n",
    "    top_k: Number of top matches to return (default: 3)\n",
    "    \"\"\"\n",
    "    \n",
    "    # preprocess the query and find related entities from the KB\n",
    "    query = preprocess_text(query)\n",
    "    query_tokens = word_tokenize(query) # tokenize words to get pos_tag and detect each if in kb\n",
    "    query_tokens = pos_tag(query_tokens) # use pos_tag here to extract the \"key words\" including Noun, verb, adjective in the query\n",
    "    query_entities = [word[0] for word in query_tokens if word[1] in ['NN','NNP','NNS','NNPS','JJ','JJR','JJS','VB','VBG','VBD','VBN','VBP','VBZ']]\n",
    "    \n",
    "    # find related entities from the KB for each token in the query\n",
    "    for token in query_tokens:\n",
    "        if token[0] in kb and token[1] in ['NN','NNP','NNS','NNPS']:# detect if the noun in the token is in the KB\n",
    "            query_entities += kb[token[0]] # add the aliases into the query_entities\n",
    "    \n",
    "    # compute the query embedding after combined associated names in kb\n",
    "    query_embedding = average_word_vectors(query_entities, model, set(model.wv.index_to_key), 200)\n",
    "    \n",
    "    # calculate the cosine similarity between the query embedding and the article embeddings\n",
    "    cos_similarities  = cosine_similarity([query_embedding], article_embeddings)[0]\n",
    "    \n",
    "    # get the indices of the top_k most similar articles\n",
    "    top_n_indices = np.argsort(cos_similarities)[-top_k:][::-1]  # Top 3 most similar articles\n",
    "    \n",
    "    # create a list of the top_k most similar articles with their similarity scores\n",
    "    top_n_similarities = cos_similarities[top_n_indices]   \n",
    "    \n",
    "    # extract the snippet from the article\n",
    "    snippet = []\n",
    "    for index in top_n_indices:\n",
    "        # read the file again to extract each part of body text\n",
    "        file = \"C:/Users/17820/Downloads/pmc_json/\"+processed_data.iloc[index]['paper_id']+'.xml.json'\n",
    "        with open(file ,encoding = 'utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        paragraph = []\n",
    "        for text in data['body_text']:\n",
    "            text = text['text']\n",
    "            sentences = preprocess_text(text) # reprocess each text in the articles to make them embedding later\n",
    "            paragraph.append(sentences)\n",
    "        \n",
    "        # compute each part of text to snippet embedding\n",
    "        snippet_embeddings = [average_word_vectors(sentences.split(), model, set(model.wv.index_to_key), 200) for sentences in paragraph]\n",
    "        \n",
    "        # calculate again the cosine similarity between the query embedding and the snippet embeddings\n",
    "        cos_similarities_snippet = cosine_similarity([query_embedding], snippet_embeddings)[0]\n",
    "        \n",
    "        # use np.argsort() funtion to let each snippet in the article be sorted according to cos_similarities_snippet from largest to smallest\n",
    "        snippet_index = np.argsort(cos_similarities_snippet)[-1:][::-1]\n",
    "        \n",
    "        # add the most related original version of snippet in each of three articles in the list\n",
    "        snippet.append(data['body_text'][snippet_index[0]]['text'])\n",
    "        \n",
    "        #csv_path = 'C:/Users/17820/Downloads/metadata.csv'\n",
    "        # prepare the three top articles' paper_ids for output\n",
    "        paper_ids = [processed_data.iloc[index]['paper_id'] for index in top_n_indices]\n",
    "        # prepare the three top articles' similarities for output\n",
    "        similarities = [cos_similarities[index] for index in top_n_indices]\n",
    "    return snippet,paper_ids,similarities \n",
    "    \n",
    "def extract_answer(query, snippet):\n",
    "    \"\"\"\n",
    "    Extract a specific answer from the snippet based on the question word.\n",
    "\n",
    "    query: The input query (question)\n",
    "    snippet: The snippet from the most relevant article\n",
    "    \"\"\"\n",
    "    query_tags = pos_tag(word_tokenize(query))\n",
    "    question_word = \"\"\n",
    "    for word, pos in query_tags:\n",
    "        if pos in [\"WDT\", \"WP\", \"WP$\", \"WRB\"]:\n",
    "            question_word = word.lower()\n",
    "            break\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(snippet)\n",
    "    if question_word in [\"who\", \"what\", \"which\"]:\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in [\"PERSON\", \"ORG\"]:\n",
    "                return ent.text\n",
    "\n",
    "    elif question_word == \"when\":\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in [\"DATE\", \"TIME\"]:\n",
    "                return ent.text\n",
    "\n",
    "    elif question_word == \"where\":\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in [\"GPE\", \"LOC\", \"FAC\"]:\n",
    "                return ent.text\n",
    "\n",
    "    elif question_word in [\"how\", \"why\"]:\n",
    "        for sent in doc.sents:\n",
    "            for token in sent:\n",
    "                if token.dep_ == \"ROOT\":\n",
    "                    return sent.text\n",
    "    \n",
    "    \n",
    "\n",
    "#In the match_articles() function I combined the method of finding the article in the pd and the text in the article, this step is also the most important step of the whole system.\n",
    "#First we extracted the noun, verb and adjective parts of the query, here I did not choose to use NER but use pos_tag because NER can not identify the key words in the question well, \n",
    "#it may be common nouns (face mask) or the most words (larggest), so extract them all can be better to get the semantics.\n",
    "#Then for each keyword in the noun, I searched in the KB whether it has aliases and added to the tokens of the query, which is very helpful to improve the accuracy of the search and increase the weight of keywords.\n",
    "#For similarity comparison I used cos_similarities to embedding the query and each atricle to get the three most matching articles, \n",
    "#and similarly for each paragraph in the atricle I used this method to find the most relevant snippet, snippet and their scores (similarity) as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20fed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "#device_available = torch.cuda.is_available()\n",
    "device_available = False\n",
    "from IPython.core.display import display, HTML\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "if device_available:\n",
    "    model.cuda()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "def get_dataset(csv_path):\n",
    "    corpus = []\n",
    "    csv_df = pd.read_csv(csv_path).dropna(subset=['authors', 'abstract']).drop_duplicates(subset='abstract')\n",
    "    csv_df = csv_df[csv_df['abstract']!='Unknown']\n",
    "    for ix,row in csv_df.iterrows():\n",
    "        if row['abstract'] and not pd.isna(row['abstract']):\n",
    "            temp_dict = dict()\n",
    "            temp_dict['abstract'] = row['abstract']\n",
    "            temp_dict['title'] = row['title']\n",
    "            temp_dict['authors'] = row['authors']\n",
    "            temp_dict['url'] = row['doi']\n",
    "            temp_dict['publish_time'] = row['publish_time']\n",
    "            corpus.append(temp_dict)\n",
    "    return corpus\n",
    "\n",
    "wrapper = textwrap.TextWrapper(width=80) \n",
    "\n",
    "corpus = get_dataset('https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-03-27/metadata.csv')\n",
    "def answer_question_dict(question, keyword=None, show_visualization=False):\n",
    "\n",
    "    '''\n",
    "    Takes a `question` string and an `answer_text` string (which contains the\n",
    "    answer), and identifies the words within the `answer_text` that are the\n",
    "    answer. Prints them out.\n",
    "    '''\n",
    "    # select corpus\n",
    "    answer_text = corpus\n",
    "\n",
    "    # Initializing answers list\n",
    "    answers = {}\n",
    "    min_score = 0\n",
    "    counter = 0 # for stopping iterations earlier\n",
    "    \n",
    "    for answer_option in answer_text:\n",
    "        if keyword and keyword not in answer_option['abstract']:\n",
    "        continue\n",
    "\n",
    "      # ======== Tokenize ========\n",
    "      # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "        input_ids = tokenizer.encode(question, answer_option['abstract'],max_length=512)\n",
    "\n",
    "      # Report how long the input sequence is.\n",
    "      #print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
    "\n",
    "      # ======== Set Segment IDs ========\n",
    "      # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "        sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "      # The number of segment A tokens includes the [SEP] token istelf.\n",
    "        num_seg_a = sep_index + 1\n",
    "\n",
    "      # The remainder are segment B.\n",
    "        num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "      # Construct the list of 0s and 1s.\n",
    "        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "    \n",
    "\n",
    "      # There should be a segment_id for every input token.\n",
    "        assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "      # ======== Evaluate ========\n",
    "      # Run our example question through the model.\n",
    "        \n",
    "        input_ids_tensor = torch.tensor([input_ids])\n",
    "        segment_ids_tensor = torch.tensor([segment_ids])\n",
    "        if device_available:\n",
    "            input_ids_tensor = input_ids_tensor.to('cuda:0')\n",
    "            segment_ids_tensor = segment_ids_tensor.to('cuda:0')\n",
    "\n",
    "        start_scores, end_scores = model(input_ids_tensor, # The tokens representing our input text.\n",
    "                                  token_type_ids=segment_ids_tensor) # The segment IDs to differentiate question from answer_text\n",
    "    \n",
    "      # only review answers with score above threshold\n",
    "        score = round(torch.max(start_scores).item(), 3)\n",
    "\n",
    "        if score>min_score and score>0:\n",
    "\n",
    "        # ======== Reconstruct Answer ========\n",
    "        \n",
    "        # Find the tokens with the highest `start` and `end` scores.\n",
    "            answer_start = torch.argmax(start_scores)\n",
    "            answer_end = torch.argmax(end_scores)\n",
    "\n",
    "\n",
    "        # Get the string versions of the input tokens.\n",
    "            tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "        # Start with the first token.\n",
    "            answer = tokens[answer_start]\n",
    "\n",
    "        # Select the remaining answer tokens and join them with whitespace.\n",
    "            for i in range(answer_start + 1, answer_end + 1):\n",
    "\n",
    "                # If it's a subword token, then recombine it with the previous token.\n",
    "                if tokens[i][0:2] == '##':\n",
    "                    answer += tokens[i][2:]\n",
    "\n",
    "                # Otherwise, add a space then the token.\n",
    "                else:\n",
    "                    answer += ' ' + tokens[i]\n",
    "\n",
    "        # ======== Add Answer to best answers list ========\n",
    "\n",
    "            if len(answers)>4:\n",
    "                min_score = min([d for d in answers.keys()])\n",
    "\n",
    "            if len(answers)==10:\n",
    "                answers.pop(min_score)\n",
    "            answers[score] = [answer, score, '<a href=\"https://doi.org/'+str(answer_option['url'])+'\" target=\"_blank\">' + str(answer_option['title']) +'</a>', answer_option['abstract'], answer_option['publish_time']]\n",
    "\n",
    "            visualization_start = max(answer_start-20,0)\n",
    "            visualization_end = min((answer_end+1)+20,len(tokens))\n",
    "            # Variables needed for graphs\n",
    "            s_scores = start_scores.cpu().detach().numpy().flatten()\n",
    "            e_scores = end_scores.cpu().detach().numpy().flatten()\n",
    "\n",
    "            # We'll use the tokens as the x-axis labels. In order to do that, they all need\n",
    "            # to be unique, so we'll add the token index to the end of each one.\n",
    "            token_labels = []\n",
    "            for (i, token) in enumerate(tokens):\n",
    "                token_labels.append('{:} - {:>2}'.format(token, i))\n",
    "            answers[score] = [answer, score, '<a href=\"https://doi.org/'+str(answer_option['url'])+'\" target=\"_blank\">' + str(answer_option['title']) +'</a>', answer_option['abstract'], answer_option['publish_time'], s_scores, e_scores, token_labels, visualization_start, visualization_end]\n",
    "\n",
    "    # Return dataframe with relevant data\n",
    "    df_columns = ['Answer', 'Confidence', 'Title', 'Abstract', 'Published', 's_scores', 'e_scores', 'token_labels', 'visualization_start', 'visualization_end']\n",
    "    df = pd.DataFrame.from_dict(answers, orient='index',columns = df_columns)\n",
    "    df.sort_values(by=['Confidence'], inplace=True, ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7ea3a2",
   "metadata": {},
   "source": [
    "### 6. Simple user interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "99cf2087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your question (or 'quit' to exit): \n",
      "Where the first case of COVID-19 was found?\n",
      "answer is China\n",
      "------------------------------------------------------------------------------\n",
      "Enter your question (or 'quit' to exit): \n",
      "When the first case of COVID-19 was found?\n",
      "answer is 26 March 2020\n",
      "------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [87]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m------------------------------------------------------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 26\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [87]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# I built a simple user input interface, so that you can freely select the question input and get the result until you type 'quit'\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter your question (or \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m to exit): \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mE:\\ANACONDA\\lib\\site-packages\\ipykernel\\kernelbase.py:1075\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1074\u001b[0m     )\n\u001b[1;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\ANACONDA\\lib\\site-packages\\ipykernel\\kernelbase.py:1120\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1117\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load data and preprocess\n",
    "    dataset_folder = \"C:/Users/17820/Downloads/pmc_json/\"\n",
    "    processed_data = read_and_process_data(dataset_folder)\n",
    "    #kb = create_kb(processed_data)\n",
    "    kb = {}\n",
    "    model, article_embeddings = create_article_embeddings(processed_data)\n",
    "    \n",
    "    # I built a simple user input interface, so that you can freely select the question input and get the result until you type 'quit'\n",
    "    while True:\n",
    "        query = input(\"Enter your question (or 'quit' to exit): \\n\")\n",
    "        if query.strip().lower() == 'quit':\n",
    "            break\n",
    "\n",
    "        snippet,paper_ids,similarities = match_articles(query, model, processed_data,article_embeddings, kb)\n",
    "        answer = []\n",
    "        for s in snippet:\n",
    "            ans = extract_answer(query,s)\n",
    "            answer.append(ans)\n",
    "        answer = Counter(answer)\n",
    "        answer = list(answer.keys())[0]\n",
    "        print(f\"answer is {answer}\")\n",
    "        print('------------------------------------------------------------------------------')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "# For my custom questions, the answer system gave some more relevant answers, in terms of improving performance, \n",
    "#using larger data, more accurate data cleaning and using pre-trained models are expected to improve the accuracy and score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a91b4",
   "metadata": {},
   "source": [
    "### 7. References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TREC-COVID Qrels 1, Retrieved from https://ir.nist.gov/covidSubmit/data.html#cumu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b784d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
