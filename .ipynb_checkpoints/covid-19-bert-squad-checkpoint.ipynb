{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Summary\n",
    "\n",
    "### 1.1 THE CHALLENGE\n",
    "\n",
    "The challenge of choice was \"What do we know about COVID-19 risk factors? What have we learned from epidemiological studies?\". A set of questions were raised in the challenge outline and we set out to find the asnwers to those questions using data analysis. We did not want to lose the focus on what was asked and decided for a clean simple yet very effective approach.\n",
    "\n",
    "### 1.2 THE APPROACH\n",
    "\n",
    "Our approach is based on selective models benchmarked in reference to the provided dataset in order to maximise the answering power and keep the results tied to the scope of the challenge. After reviewing a series of references referring to cuttin-edge methods in the field, we compared benchmarks in various papers and decided to find the best BERT model for this purpose. We tried several models, found the best results using a specific BERT model fine tuned for questions-answers.\n",
    "\n",
    "Other submissions picked this model, but the peculiarity that we found extremely effective was that the other solutions did not apply extensively the model, in particular decided to first filte the papers database by searching keywords and strings, and only apply BERT to find the answer in the reulting filtered data. Our approach flips this logic upside down, since we first identify the best answers across all papers database, and only after filter by keyword where needed.\n",
    "\n",
    "### 1.3 THE RESULTS\n",
    "Our results provide satisfying answers for 7 out of 8 questions. The time taken to extract an answer is reduced to a less than 30 seconds per question on a faster machine but we decided to pre-calculate them because of the limited power available on Kaggle notebooks. The summary of the results - which can be found in extended form in the relative section - follow:\n",
    "\n",
    "<table width=\"100%\">\n",
    "    <thead>\n",
    "    <tr>\n",
    "        <td width=\"30%\"  style=\"text-align:left\"> Question </td>\n",
    "        <td  style=\"text-align:left\"> Answer </td>\n",
    "    </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\"> Risk factors: Smoking, pre-existing pulmonary disease. </td> \n",
    "        <td style=\"text-align:left\"> Very likely but not demonstrated. It was demonstrated that smoking has no clinical or preventive significance for risk of influenza in the elderly. However, asthma is to be considered a risk factor for rhinovirus, which in turn often exaplains more underlying pulmonary illness. </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\"> Risk factors: Co-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other co-morbidities. </td>\n",
    "        <td style=\"text-align:left\"> Likely not. It was observed that very often infected individuals with The COVID-19 coronavirus but without any symptoms could still transfer the virus to others . However, for pneumonia in relation to viruses similar to Covid-19, i.e. HCoV-NL63 and influenza A/H1N1, it was observed that co-infections caused significantly higher rates of breathing difficulties, cough, and sore throat than those of single infections. </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\"> Neonates and pregnant women. </td>\n",
    "        <td style=\"text-align:left\"> Likely. While multiple studies demonstrated that pregnant women are more likely to develop severe illness after infection with Covid-19 and other influenza viruses, there is no study demonstrating that Covid-19 poses higher risk for neonates. However, neonates are thought to be susceptible to the virus, because their immune system is not well developed. </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\"> Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences. </td>\n",
    "        <td style=\"text-align:left\"> Likely but not demonstrated. While no study looked directly at socio-economic and behavioral factors in relation to Covid-19, previous studies focused on risk factors for population from poorer areas of the World, identifying different health risks when compared to local population - such as other infectious diseases, inadequate immunity to vaccine-preventable illnesses, higher likelihood of having malnutrition and developmental delay and psychological trauma. </td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td style=\"text-align:left\"> Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors. </td>\n",
    "        <td style=\"text-align:left\"> Covid-19 is spread by human-to-human transmission via direct contact, droplets and aerosol transmissions, and infection has been estimated to have mean incubation period of 6.4 days and a basic reproduction number of 2.24â€“3.58. With regards to enviromental factors, tests on virus surrogaes for Covid demonstrated that low air temperature and low humidity are likely to increase virus survival on surfaces up to 28 days, with studies going as far as suggesting hospitals to keep high temperatures at relatively high RH to reduce the survival of airborne influenza virus. It is also demonstrated that an enviroment that allows for substantial undocumented infection facilitates the rapid dissemination of COVID-19, while after travel restrictions and control measures are imposed the reproductive number number falls considerably. Also, testing strategies that do not produce repeated false-negatives as the existing test do are deemed beneficial for the containment of the virus. </td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td style=\"text-align:left\"> Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups. </td>\n",
    "        <td style=\"text-align:left\"> The risk of fatality among infected individuals is 0.3% to 0.6%, but among hospitalised cases this increases to 14%. Based on deaths reported, high risk groups are pregnant women, older than the general population (mean age 45),and people in hospices. </td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td style=\"text-align:left\"> Susceptibility of populations. </td>\n",
    "        <td style=\"text-align:left\"> Unkonwn </td>\n",
    "    </tr> \n",
    "        <tr>\n",
    "        <td style=\"text-align:left\"> Public health mitigation measures that could be effective for control. </td>\n",
    "        <td style=\"text-align:left\"> Although little data is available for Covid-19 control measures, based on a forecasting model on Covid-19 data, it has been showed to be effective closin borders, schools, and suspending community services and commuters. More in general, in a number of recent studies, it has been shown how different intervention measures like travel restrictions, school closures, treatment and prophylaxis might allow to control outbreaks of diseases, such as SARS, pandemic influenza and others. It is also worth noting that because the elevated death risk estimates from Covid-19 are likely associated with a breakdown of the medical/health system, enhanced public health interventions including social distancing and movement restrictions should be effectively implemented to bring the epidemic under control.  </td>\n",
    "    </tr> \n",
    "       \n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "# 2. Methodology\n",
    "\n",
    "### 2.1 Data preparation\n",
    "\n",
    "We first explored the dataset as well as clustering techniques, taking inspiration from Carrot [https://search.carrot2.org/#/pubmed]. However we decided to focus on actually providing the answers to the very specific questions that the challenge requested. Therefore, we first broke the challenge questions into a more specific set of questions that we could use to compare models and obtain overall better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 1: What do we know about COVID-19 risk factors?\n",
    "# Task details: What do we know about COVID-19 risk factors? What have we learned from epidemiological studies?\n",
    "# https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks?taskId=558\n",
    "\n",
    "questions = [{'question':\"Is smoking a risk factor?\",'keyword':None},\n",
    "             {'question':\"Is a pre-existing pulmonary disease a risk factor?\",'keyword':None},\n",
    "             {'question':\"Do co-existing conditions make the virus more transmissible?\",'keyword':None},\n",
    "             {'question':\"Is being a pregnant woman a risk factor?\",'keyword':'pregnant'},\n",
    "             {'question':\"Is being a neonate a risk factor?\",'keyword':'neonate'},\n",
    "             {'question':\"Are there differences in risk factors associated to socio-economic factors?\",'keyword':None},\n",
    "             {'question':\"How does the transmission happen?\",'keyword':'transmission'},\n",
    "             {'question':\"What is the reproductive rate?\",'keyword':None},\n",
    "             {'question':\"What is the incubation period?\",'keyword':None},\n",
    "             {'question':\"What are the modes of transmission?\",'keyword':None},\n",
    "             {'question':\"What are the enviromental factors?\",'keyword':None},\n",
    "             {'question':\"How long is the serial interval?\",'keyword':None},\n",
    "             {'question':\"What is the severity of disease among high risk groups and patients?\",'keyword':None},\n",
    "             {'question':\"What is the risk of death among high risk groups and patients?\",'keyword':None},\n",
    "             {'question':\"What is the susceptibility of populations?\",'keyword':None},\n",
    "             {'question':\"What are the public health mitigation measures that could be effective for control?\",'keyword':None}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model selection\n",
    "\n",
    "Then, we looked at various models trained on different datasets - including SBERT [https://arxiv.org/pdf/1908.10084.pdf] and SCIBERT model [https://github.com/allenai/scibert] - and we compared results across the questions proposed. We compared results by manually veriying the top papers picked by each method and scoring them from 1 to 5. Eventually we set for a BERT model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of the hidden-states output to compute span start logits and span end logits). \n",
    "\n",
    "\n",
    "### 2.2 Detailed Methodology (Code)\n",
    "\n",
    "The detailed methodology follows, including the code for each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise environment\n",
    "\n",
    "We first intall the packages needed, get the papers corpus and load the local files, including the models that we run on another machine to save time to execution on the Kaggle machine. If the files for the models not in the folder, this same code will generate the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.28.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (3.11.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2022.12.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w4/xhd6ygzn6_n2rl2603vs0m540000gn/T/ipykernel_53946/3850349858.py:8: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "#device_available = torch.cuda.is_available()\n",
    "device_available = False\n",
    "from IPython.core.display import display, HTML\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "#sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "if device_available:\n",
    "    model.cuda()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just to load the files needed instead of running the model each time.\n",
    "\n",
    "import pickle\n",
    "\n",
    "def load_or_run_answer_question_dict(question,keyword):\n",
    "    pickle_name = question.replace(' ','_').replace('?','_')\n",
    "    path_to_file = F\"/kaggle/input/kaggle/{pickle_name}.pickle\"\n",
    "    print(path_to_file)\n",
    "    try:\n",
    "      df = pickle.load(open(path_to_file, \"rb\"))\n",
    "    except (OSError, IOError) as e:\n",
    "        df = answer_question_dict(question, keyword)\n",
    "        pickle.dump(df, open(path_to_file, \"wb\"))\n",
    "    return df\n",
    "# print(os.listdir(\"../input\"))\n",
    "# print(os.listdir(\"../input/datacompetition\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def get_dataset(csv_path):\n",
    "    corpus = []\n",
    "    csv_df = pd.read_csv(csv_path).dropna(subset=['authors', 'abstract']).drop_duplicates(subset='abstract')\n",
    "    csv_df = csv_df[csv_df['abstract']!='Unknown']\n",
    "    for ix,row in csv_df.iterrows():\n",
    "        if row['abstract'] and not pd.isna(row['abstract']):\n",
    "            temp_dict = dict()\n",
    "            temp_dict['abstract'] = row['abstract']\n",
    "            temp_dict['title'] = row['title']\n",
    "            temp_dict['authors'] = row['authors']\n",
    "            temp_dict['url'] = row['doi']\n",
    "            temp_dict['publish_time'] = row['publish_time']\n",
    "            corpus.append(temp_dict)\n",
    "    return corpus\n",
    "\n",
    "wrapper = textwrap.TextWrapper(width=80) \n",
    "\n",
    "corpus = get_dataset('metadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up question-answer algorithm\n",
    "\n",
    "Here we define our main answering function. As you can see, we are scoring every token in every abstract and not just a pre-selected few papers or abstracts like in the other submissions we saw. This allows us to obtain the full database scored, which we then rank and return only the top results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_dict(question, keyword=None, show_visualization=False):\n",
    "\n",
    "    '''\n",
    "    Takes a `question` string and an `answer_text` string (which contains the\n",
    "    answer), and identifies the words within the `answer_text` that are the\n",
    "    answer. Prints them out.\n",
    "    '''\n",
    "    # select corpus\n",
    "    answer_text = corpus\n",
    "\n",
    "    # Initializing answers list\n",
    "    answers = {}\n",
    "    min_score = 0\n",
    "    counter = 0 # for stopping iterations earlier\n",
    "    \n",
    "    for answer_option in answer_text:\n",
    "      if keyword and keyword not in answer_option['abstract']:\n",
    "        continue\n",
    "\n",
    "      # ======== Tokenize ========\n",
    "      # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "      input_ids = tokenizer.encode(question, answer_option['abstract'],max_length=512)\n",
    "\n",
    "      # Report how long the input sequence is.\n",
    "      #print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
    "\n",
    "      # ======== Set Segment IDs ========\n",
    "      # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "      sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "      # The number of segment A tokens includes the [SEP] token istelf.\n",
    "      num_seg_a = sep_index + 1\n",
    "\n",
    "      # The remainder are segment B.\n",
    "      num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "      # Construct the list of 0s and 1s.\n",
    "      segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "    \n",
    "\n",
    "      # There should be a segment_id for every input token.\n",
    "      assert len(segment_ids) == len(input_ids)\n",
    "      \n",
    "\n",
    "      # ======== Evaluate ========\n",
    "      # Run our example question through the model.\n",
    "        \n",
    "      input_ids_tensor = torch.tensor([input_ids])\n",
    "      segment_ids_tensor = torch.tensor([segment_ids])\n",
    "      if device_available:\n",
    "         input_ids_tensor = input_ids_tensor.to('cuda:0')\n",
    "         segment_ids_tensor = segment_ids_tensor.to('cuda:0')\n",
    "\n",
    "      start_scores, end_scores = model(input_ids_tensor, # The tokens representing our input text.\n",
    "                                  token_type_ids=segment_ids_tensor) # The segment IDs to differentiate question from answer_text\n",
    "    \n",
    "      # only review answers with score above threshold\n",
    "      score = round(torch.max(start_scores).item(), 3)\n",
    "\n",
    "      if score>min_score and score>0:\n",
    "\n",
    "        # ======== Reconstruct Answer ========\n",
    "        \n",
    "        # Find the tokens with the highest `start` and `end` scores.\n",
    "        answer_start = torch.argmax(start_scores)\n",
    "        answer_end = torch.argmax(end_scores)\n",
    "\n",
    "\n",
    "        # Get the string versions of the input tokens.\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "        # Start with the first token.\n",
    "        answer = tokens[answer_start]\n",
    "\n",
    "        # Select the remaining answer tokens and join them with whitespace.\n",
    "        for i in range(answer_start + 1, answer_end + 1):\n",
    "            \n",
    "            # If it's a subword token, then recombine it with the previous token.\n",
    "            if tokens[i][0:2] == '##':\n",
    "                answer += tokens[i][2:]\n",
    "            \n",
    "            # Otherwise, add a space then the token.\n",
    "            else:\n",
    "                answer += ' ' + tokens[i]\n",
    "\n",
    "        # ======== Add Answer to best answers list ========\n",
    "\n",
    "        if len(answers)>4:\n",
    "          min_score = min([d for d in answers.keys()])\n",
    "          \n",
    "        if len(answers)==10:\n",
    "          answers.pop(min_score)\n",
    "        answers[score] = [answer, score, '<a href=\"https://doi.org/'+str(answer_option['url'])+'\" target=\"_blank\">' + str(answer_option['title']) +'</a>', answer_option['abstract'], answer_option['publish_time']]\n",
    "        \n",
    "        visualization_start = max(answer_start-20,0)\n",
    "        visualization_end = min((answer_end+1)+20,len(tokens))\n",
    "        # Variables needed for graphs\n",
    "        s_scores = start_scores.cpu().detach().numpy().flatten()\n",
    "        e_scores = end_scores.cpu().detach().numpy().flatten()\n",
    "        \n",
    "        # We'll use the tokens as the x-axis labels. In order to do that, they all need\n",
    "        # to be unique, so we'll add the token index to the end of each one.\n",
    "        token_labels = []\n",
    "        for (i, token) in enumerate(tokens):\n",
    "            token_labels.append('{:} - {:>2}'.format(token, i))\n",
    "        answers[score] = [answer, score, '<a href=\"https://doi.org/'+str(answer_option['url'])+'\" target=\"_blank\">' + str(answer_option['title']) +'</a>', answer_option['abstract'], answer_option['publish_time'], s_scores, e_scores, token_labels, visualization_start, visualization_end]\n",
    "        \n",
    "    # Return dataframe with relevant data\n",
    "    df_columns = ['Answer', 'Confidence', 'Title', 'Abstract', 'Published', 's_scores', 'e_scores', 'token_labels', 'visualization_start', 'visualization_end']\n",
    "    df = pd.DataFrame.from_dict(answers, orient='index',columns = df_columns)\n",
    "    df.sort_values(by=['Confidence'], inplace=True, ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Results & Discussion\n",
    "\n",
    "### 3.1 Results in full\n",
    "\n",
    "Here the questions defined in the sections above are answered and results are presented in full. All answers in the results above come from these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================\n",
      "{'question': 'Is smoking a risk factor?', 'keyword': None}\n",
      "/kaggle/input/kaggle/Is_smoking_a_risk_factor_.pickle\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "max(): argument 'input' (position 1) must be Tensor, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m, in \u001b[0;36mload_or_run_answer_question_dict\u001b[0;34m(question, keyword)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m   df \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath_to_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mIOError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/kaggle/Is_smoking_a_risk_factor_.pickle'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m======================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(question)\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mload_or_run_answer_question_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeyword\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m display(HTML(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfidence\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbstract\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPublished\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_html(render_links\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, escape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m======================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mload_or_run_answer_question_dict\u001b[0;34m(question, keyword)\u001b[0m\n\u001b[1;32m     10\u001b[0m   df \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(path_to_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mIOError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 12\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43manswer_question_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(df, \u001b[38;5;28mopen\u001b[39m(path_to_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "Cell \u001b[0;32mIn[6], line 58\u001b[0m, in \u001b[0;36manswer_question_dict\u001b[0;34m(question, keyword, show_visualization)\u001b[0m\n\u001b[1;32m     54\u001b[0m start_scores, end_scores \u001b[38;5;241m=\u001b[39m model(input_ids_tensor, \u001b[38;5;66;03m# The tokens representing our input text.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m                             token_type_ids\u001b[38;5;241m=\u001b[39msegment_ids_tensor) \u001b[38;5;66;03m# The segment IDs to differentiate question from answer_text\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# only review answers with score above threshold\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_scores\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score\u001b[38;5;241m>\u001b[39mmin_score \u001b[38;5;129;01mand\u001b[39;00m score\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m   \u001b[38;5;66;03m# ======== Reconstruct Answer ========\u001b[39;00m\n\u001b[1;32m     63\u001b[0m   \n\u001b[1;32m     64\u001b[0m   \u001b[38;5;66;03m# Find the tokens with the highest `start` and `end` scores.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m   answer_start \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(start_scores)\n",
      "\u001b[0;31mTypeError\u001b[0m: max(): argument 'input' (position 1) must be Tensor, not str"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    print(\"======================\")\n",
    "    print(question)\n",
    "    df = load_or_run_answer_question_dict(question['question'], question['keyword'])\n",
    "    display(HTML(df[['Answer', 'Confidence', 'Title', 'Abstract', 'Published']].to_html(render_links=True, escape=False, index=False)))\n",
    "    print(\"======================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Discussion\n",
    "\n",
    "We believe our method is a novel approach to this challenge and it allowed to answer to the posed questions in an accurate and clear manner. The code is simple, the results are surfaced and can be used easily even by non-experts without too much effort.\n",
    "\n",
    "There are a few gaps in the answers - in particular to susceptible population - this is an interesting point about data gaps because we do know there are publications out there addressing this questions (e.g. https://www.sciencedirect.com/science/article/pii/S0896841120300469 ) but they are not in the database and therefore the algorithm did not surface them. Also, a lot of the data is unstructured and not parsed - in particular in the full text form.\n",
    "\n",
    "Also, the database is very heterogenous and the results are often relative to old research or other viruses or outbreaks - rhinovirus, Sars etc - presenting the risk of oversimplification and generalization of the findings that the careful reader can identify when looking at the best set of results, but the models (not ours, all of those that we checked) will struggle to grasp. This could lead to really dangerous results and therefore we do think this tools should be used just to rank the findings and surface the possible answers and then - as demonstrated in the Summary above - it'd be very simple for a reader to pick the best answer. This is often based on the full abstract, the title and the year of publication, so we included those elements (as well as a link to the full text) in our results output.\n",
    "\n",
    "In terms of models parameters - different scoring methods affect the results of the model. Therefore, we assessed the scores and compared results using two different scoring methods - first using only the starting token score, and then using a composite score based on both start and end tokens. An exemplificative set of tokens scores for the first question is showed in the figures below - we found that by taking the highest scores in a 40 tokens range around the start (green spike in the first figure below) and end (green spike in the second figure below) token gives the best answers, so we implemented that in the question/answer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/kaggle/Is_smoking_a_risk_factor_.pickle\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "max(): argument 'input' (position 1) must be Tensor, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m, in \u001b[0;36mload_or_run_answer_question_dict\u001b[0;34m(question, keyword)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m   df \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath_to_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mIOError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/kaggle/Is_smoking_a_risk_factor_.pickle'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m   plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnd Word Scores [for first answer]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m   plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 15\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mload_or_run_answer_question_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeyword\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m start_word_plot(df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_labels\u001b[39m\u001b[38;5;124m'\u001b[39m],df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms_scores\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     17\u001b[0m end_word_plot(df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_labels\u001b[39m\u001b[38;5;124m'\u001b[39m],df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124me_scores\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mload_or_run_answer_question_dict\u001b[0;34m(question, keyword)\u001b[0m\n\u001b[1;32m     10\u001b[0m   df \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(path_to_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mIOError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 12\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43manswer_question_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(df, \u001b[38;5;28mopen\u001b[39m(path_to_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "Cell \u001b[0;32mIn[6], line 58\u001b[0m, in \u001b[0;36manswer_question_dict\u001b[0;34m(question, keyword, show_visualization)\u001b[0m\n\u001b[1;32m     54\u001b[0m start_scores, end_scores \u001b[38;5;241m=\u001b[39m model(input_ids_tensor, \u001b[38;5;66;03m# The tokens representing our input text.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m                             token_type_ids\u001b[38;5;241m=\u001b[39msegment_ids_tensor) \u001b[38;5;66;03m# The segment IDs to differentiate question from answer_text\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# only review answers with score above threshold\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_scores\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score\u001b[38;5;241m>\u001b[39mmin_score \u001b[38;5;129;01mand\u001b[39;00m score\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m   \u001b[38;5;66;03m# ======== Reconstruct Answer ========\u001b[39;00m\n\u001b[1;32m     63\u001b[0m   \n\u001b[1;32m     64\u001b[0m   \u001b[38;5;66;03m# Find the tokens with the highest `start` and `end` scores.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m   answer_start \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(start_scores)\n",
      "\u001b[0;31mTypeError\u001b[0m: max(): argument 'input' (position 1) must be Tensor, not str"
     ]
    }
   ],
   "source": [
    "def start_word_plot(token_labels, s_scores):\n",
    "  ax = sns.barplot(x=token_labels, y=s_scores, ci=None)\n",
    "  ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
    "  ax.grid(True)\n",
    "  plt.title('Start Word Scores [for first answer]')\n",
    "  plt.show()\n",
    "\n",
    "def end_word_plot(token_labels,e_scores):\n",
    "  ax = sns.barplot(x=token_labels, y=e_scores, ci=None)\n",
    "  ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
    "  ax.grid(True)\n",
    "  plt.title('End Word Scores [for first answer]')\n",
    "  plt.show()\n",
    "\n",
    "df = load_or_run_answer_question_dict(questions[0]['question'], question['keyword'])\n",
    "start_word_plot(df.iloc[0]['token_labels'],df.iloc[0]['s_scores'])\n",
    "end_word_plot(df.iloc[0]['token_labels'],df.iloc[0]['e_scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Pros and cons and future work\n",
    "\n",
    "We think that our model is accurate and can surface the best answers. The mode is however not fast on a chaep machine, and needs some pre-training on faster machines. Also, it can be largely be improved - our plans for the next submission is to proceed with a split of the answers by outbreak and viruses, as it seems that many papers related to rhinovirus, SARS-CoV-2 or other respiratory pathogen; also we plan to extend the answers model to the full text of the papers, as well as provide more guidance to the doctors on how to pick the best answers in the surfaced set.\n",
    "\n",
    "# 4. REUSABILITY\n",
    "---\n",
    "The code needed to train and run the models is all in this file, as well as the results. To add a question, simply add it to the list of questions in section 2.1, and re-run the remaining of the code. \n",
    "\n",
    "\n",
    "# 5. REFERENCES\n",
    "---\n",
    "\n",
    "Scoring and other notes we used: \n",
    "\n",
    "https://docs.google.com/spreadsheets/d/1PKQVCSBK2Xsuvh_oLbQkejBDI0Jve23hJCeDMkYWu80/edit?ts=5e7a9121#gid=164381334\n",
    "\n",
    "\n",
    "Other references that we used:\n",
    "\n",
    "https://arxiv.org/pdf/1908.10084.pdf\n",
    "\n",
    "https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46808.pdf\n",
    "\n",
    "https://search.carrot2.org/#/pubmed\n",
    "\n",
    "https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/\n",
    "\n",
    "https://arxiv.org/pdf/1512.01337.pdf\n",
    "\n",
    "https://www.aclweb.org/anthology/D14-1070.pdf\n",
    "\n",
    "https://arxiv.org/abs/1903.10676\n",
    "\n",
    "https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "https://rajpurkar.github.io/SQuAD-explorer/\n",
    "\n",
    "https://arxiv.org/pdf/1810.04805.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2740c9a70724470f9d5f4518248123fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_eee19c43d1fd40b7895a05ac914fb080",
        "IPY_MODEL_cf3ecd55964f43eb9ad094576c873690"
       ],
       "layout": "IPY_MODEL_6c2f5c91c95b42fb9f6cd4c0e86a2628"
      }
     },
     "41907e7fd64d49c894dd8df423e30e5c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "4dfa21013a914bf6b64048031ef41ec1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ff7f549ce8674dafb3a832c5a0568da3",
       "max": 1340675298,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_41907e7fd64d49c894dd8df423e30e5c",
       "value": 1340675298
      }
     },
     "5abdecac426646ada9dc65351c4c2402": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5af9671ff1d341149be6fc0eadd332bb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "63d7e94e99754616be20491fbcf1bf9e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e501d2aff83646d283ab797c5a20acfd",
        "IPY_MODEL_686f339bdeb249f084c51a6db9714689"
       ],
       "layout": "IPY_MODEL_681141849c0a4a85a3356f00debc2287"
      }
     },
     "681141849c0a4a85a3356f00debc2287": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "686f339bdeb249f084c51a6db9714689": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5af9671ff1d341149be6fc0eadd332bb",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_6d6c941fae8d4be6987c5b8061f993d0",
       "value": " 232k/232k [00:00&lt;00:00, 2.12MB/s]"
      }
     },
     "6c2f5c91c95b42fb9f6cd4c0e86a2628": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6d6c941fae8d4be6987c5b8061f993d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "712454daefbf4fb489f72069c25cad66": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "75c6de1713684cb6b4cb34187d289f73": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4dfa21013a914bf6b64048031ef41ec1",
        "IPY_MODEL_acc11c7a669d4d4da355eeb093ffcdd4"
       ],
       "layout": "IPY_MODEL_f1f8c8a2d6d8469492e4035aecd052e7"
      }
     },
     "8ebc639a68d54ebaa32c0df803e6b402": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9c4ec3e4a0e0488faeef121adbba5149": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "9c802dc3f0df4ac8bc4c744a0b741195": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "acc11c7a669d4d4da355eeb093ffcdd4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_712454daefbf4fb489f72069c25cad66",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_9c802dc3f0df4ac8bc4c744a0b741195",
       "value": " 1.34G/1.34G [00:37&lt;00:00, 35.7MB/s]"
      }
     },
     "b49fccc5a871402699e1a43602400503": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "bb8629fd21d941fcb5c502de336c84a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cf3ecd55964f43eb9ad094576c873690": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8ebc639a68d54ebaa32c0df803e6b402",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_f03430c2a5384bab89265e9172b9458f",
       "value": " 398/398 [01:07&lt;00:00, 5.90B/s]"
      }
     },
     "e501d2aff83646d283ab797c5a20acfd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5abdecac426646ada9dc65351c4c2402",
       "max": 231508,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b49fccc5a871402699e1a43602400503",
       "value": 231508
      }
     },
     "eee19c43d1fd40b7895a05ac914fb080": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bb8629fd21d941fcb5c502de336c84a9",
       "max": 398,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9c4ec3e4a0e0488faeef121adbba5149",
       "value": 398
      }
     },
     "f03430c2a5384bab89265e9172b9458f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f1f8c8a2d6d8469492e4035aecd052e7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ff7f549ce8674dafb3a832c5a0568da3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
