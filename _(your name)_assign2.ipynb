{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6730fecf",
   "metadata": {},
   "source": [
    "## Assignment 2 \n",
    "### Jiaxuan  a1876484"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df2aac6",
   "metadata": {},
   "source": [
    "### 1. Reading dataset and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e967e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text input for both dataset articles and queries.\n",
    "\n",
    "    text: Text in string.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer() # define lemmatizer\n",
    "    stop_words = set(stopwords.words('english')) # use stop_words\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t.lower() for t in tokens if t.lower() not in stop_words] # eliminate the stopwords\n",
    "    tokens = [t for t in tokens if t not in string.punctuation] # eliminate the stopwords\n",
    "    tokens = [t for t in tokens if not t.isnumeric()] # eliminate the single numbers to clean the text, keep the words like COVID-19.\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens] # lemmatize rest words\n",
    "    return \" \".join(tokens) # return a string after preprocess\n",
    "\n",
    "\n",
    "def read_and_process_data(file_path, num_samples = 1000):\n",
    "    \"\"\"\n",
    "    Randomly load and process the dataset and save in a dataframe in the end.\n",
    "\n",
    "    file_path: Path to the folder containing the JSON files\n",
    "    num_samples: Number of samples to read from the folder (default: 10000)\n",
    "    \"\"\"\n",
    "    files = os.listdir(file_path) # get all the file names in the folder\n",
    "    sample = random.sample(files, num_samples) # randomly pick files with the number of samples\n",
    "    df_sample = pd.DataFrame(columns=['paper_id', 'paragraphs'])# create a dataframe to save the aritcle id and paragraphs \n",
    "    for file in sample: # read files, extract paper_id and texts in bodytext\n",
    "        with open(file_path + file ,encoding = 'utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        paperid = data['paper_id']\n",
    "        bodytext = ''\n",
    "        for text in data['body_text']:\n",
    "            text = text['text']\n",
    "            bodytext += text\n",
    "        df_sample.loc[len(df_sample.index)] = [paperid , bodytext] # add a line with paper_id and all the bodytext in it.\n",
    "    df_sample[\"paragraphs\"] = df_sample[\"paragraphs\"].apply(preprocess_text)# preprocess all the paragraphs.\n",
    "    return df_sample\n",
    "# Here we decided to use a dataframe to save the text as sentences, because we will use word2vec to transform them into word embeddings, \n",
    "# so the \"distance\" between them is very helpful and necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d75afc",
   "metadata": {},
   "source": [
    "### 2. Named Entity Recognition and Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d286790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_knowledge_base(articles):\n",
    "    \"\"\"\n",
    "    Use NER to create KB, extract all the NEs that are linked by 'also named', 'also called' and 'also known as'\n",
    "\n",
    "    articles: Articles after pre-process\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_sci_sm\") # more accurate extraction of NER using spcay's medical data\n",
    "    kb = {}\n",
    "    def add_to_kb(key: str, value: str): # define adding the detected words to the dictionary\n",
    "        if key not in kb:\n",
    "            kb[key] = [value]\n",
    "        elif value not in kb[key]:\n",
    "            kb[key].append(value)\n",
    "\n",
    "    for _, paragraphs in articles:\n",
    "        doc = nlp(paragraphs)\n",
    "        for token in doc:\n",
    "            # check for aliases using \"also/sometines known as\" pattern\n",
    "            if token.lower_ in {\"also\", \"sometimes\"}:\n",
    "                conj = token.head\n",
    "                if conj.lower_ in {\"known\",\"called\",\"named\"}:\n",
    "                    subject = conj.head\n",
    "                    obj = [t for t in conj.rights if t.dep_ in {\"dobj\", \"attr\"}]\n",
    "                    # add the word after lemmatizer and lower\n",
    "                    add_to_kb(subject.lemma_.lower(), obj[0].lemma_.lower()) \n",
    "\n",
    "            # check for aliases using \"or\" or \"cc\"pattern\n",
    "            if token.lower_ == \"or\" and token.dep_ == \"cc\":\n",
    "                left = token.lefts\n",
    "                right = token.rights\n",
    "                for left_item in left:\n",
    "                    for right_item in right:\n",
    "                        # determine whether the left and right lexicals are consistent with pos_tag\n",
    "                        if (left_item.dep_ in {\"conj\", \"appos\"} and\n",
    "                            right_item.dep_ in {\"conj\", \"appos\"} and\n",
    "                            left_item.pos_ in {\"NOUN\", \"PROPN\"} and\n",
    "                            right_item.pos_ in {\"NOUN\", \"PROPN\"}):\n",
    "                            # add the word after lemmatizer and lower\n",
    "                            add_to_kb(left_item.lemma_.lower(), right_item.lemma_.lower())\n",
    "\n",
    "    return kb\n",
    "\n",
    "#Here I used the simpler method of finding conjunctions to find aliases, \n",
    "#but to be more precise I added a lexical determination so that only nouns would be added to the KB dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333cefa8",
   "metadata": {},
   "source": [
    "### 3. Indexing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e5d6ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Article embeddings\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \"\"\"\n",
    "    Calculate the average word vectors for a list of words.\n",
    "\n",
    "    words: The input list of words\n",
    "    model: Word2Vec model trained on the paragraphs\n",
    "    vocabulary: Set of words in the model's vocabulary\n",
    "    num_features: Size of the word embeddings same to the model size\n",
    "    \"\"\"\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float32\") # initialize an empty feature vector with the same size as the word embeddings\n",
    "    n_words = 0 # initialize a counter for the number of words in the list that are in the model's vocabulary\n",
    "    for word in words: # iterate through the words in the list\n",
    "         \n",
    "        if word in vocabulary: # if the word is in the model's vocabulary, update the feature vector and counter\n",
    "            n_words += 1\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "            \n",
    "    # if there are any valid words in the list, divide the feature vector by the number of valid words\n",
    "    if n_words:\n",
    "        feature_vector = np.divide(feature_vector, n_words)\n",
    "    return feature_vector\n",
    "\n",
    "def create_article_embeddings(processed_data, embedding_size=200):\n",
    "    \"\"\"\n",
    "    Create article embeddings using Word2Vec.\n",
    "\n",
    "    processed_data: DataFrame containing the processed articles\n",
    "    embedding_size: Size of the word embeddings (default: 200)\n",
    "    \"\"\"\n",
    "    # train the Word2Vec model\n",
    "    #model = Word2Vec([paragraph.split() for paragraph in processed_data[\"paragraphs\"]], vector_size=embedding_size, min_count=3,window=5,workers=4)\n",
    "    model = Word2Vec.load(\"word2vec.model\")\n",
    "    # compute the average embeddings for each article\n",
    "    article_embeddings = [average_word_vectors(article.split(), model, set(model.wv.index_to_key), 200) for article in processed_data[\"paragraphs\"]]\n",
    "    #model.save(\"word2vec.model\")\n",
    "    return model, article_embeddings\n",
    "# Here I use Inverted Index to implement article indexing, in average_word_vectors() we define the implementation of converting text into word vectors, \n",
    "# then generate the model and define the word vector output of all articles separately in create_article_embeddings() to compare later with word vecters of query words, \n",
    "# And I use the content of 20,000 files as the training content of my model, the size of the model is about 650MB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067cebcf",
   "metadata": {},
   "source": [
    "### 4. Text matching utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c292820d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4. Text matching utility\n",
    "\n",
    "    \n",
    "def match_articles(query, model,processed_data, article_embeddings, kb, top_k=10):\n",
    "    \"\"\"\n",
    "    Match articles based on the given query.\n",
    "\n",
    "    query: The input query (question)\n",
    "    model: Word2Vec model trained on the paragraphs\n",
    "    processed_data: The dataframe of dataset to find the cord_uid\n",
    "    article_embeddings: Precomputed article embeddings\n",
    "    kb: Knowledge Base containing named entities and their associated terms\n",
    "    top_k: Number of top matches to return (default: 3)\n",
    "    \"\"\"\n",
    "    \n",
    "    # preprocess the query and find related entities from the KB\n",
    "    query = preprocess_text(query)\n",
    "    query_tokens = word_tokenize(query) # tokenize words to get pos_tag and detect each if in kb\n",
    "    query_tokens = pos_tag(query_tokens) # use pos_tag here to extract the \"key words\" including Noun, verb, adjective in the query\n",
    "    query_entities = [word[0] for word in query_tokens if word[1] in ['NN','NNP','NNS','NNPS','JJ','JJR','JJS','VB','VBG','VBD','VBN','VBP','VBZ']]\n",
    "    \n",
    "    # find related entities from the KB for each token in the query\n",
    "    for token in query_tokens:\n",
    "        if token[0] in kb and token[1] in ['NN','NNP','NNS','NNPS']:# detect if the noun in the token is in the KB\n",
    "            query_entities += kb[token[0]] # add the aliases into the query_entities\n",
    "    \n",
    "    # compute the query embedding after combined associated names in kb\n",
    "    query_embedding = average_word_vectors(query_entities, model, set(model.wv.index_to_key), 200)\n",
    "    \n",
    "    # calculate the cosine similarity between the query embedding and the article embeddings\n",
    "    cos_similarities  = cosine_similarity([query_embedding], article_embeddings)[0]\n",
    "    \n",
    "    # get the indices of the top_k most similar articles\n",
    "    top_n_indices = np.argsort(cos_similarities)[-top_k:][::-1]  # Top 3 most similar articles\n",
    "    \n",
    "    # create a list of the top_k most similar articles with their similarity scores\n",
    "    top_n_similarities = cos_similarities[top_n_indices]   \n",
    "    \n",
    "    # extract the snippet from the article\n",
    "    snippet = []\n",
    "    for index in top_n_indices:\n",
    "        # read the file again to extract each part of body text\n",
    "        file = \"C:/Users/17820/Downloads/pmc_json/\"+processed_data.iloc[index]['paper_id']+'.xml.json'\n",
    "        with open(file ,encoding = 'utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        paragraph = []\n",
    "        for text in data['body_text']:\n",
    "            text = text['text']\n",
    "            sentences = preprocess_text(text) # reprocess each text in the articles to make them embedding later\n",
    "            paragraph.append(sentences)\n",
    "        \n",
    "        # compute each part of text to snippet embedding\n",
    "        snippet_embeddings = [average_word_vectors(sentences.split(), model, set(model.wv.index_to_key), 200) for sentences in paragraph]\n",
    "        \n",
    "        # calculate again the cosine similarity between the query embedding and the snippet embeddings\n",
    "        cos_similarities_snippet = cosine_similarity([query_embedding], snippet_embeddings)[0]\n",
    "        \n",
    "        # use np.argsort() funtion to let each snippet in the article be sorted according to cos_similarities_snippet from largest to smallest\n",
    "        snippet_index = np.argsort(cos_similarities_snippet)[-1:][::-1]\n",
    "        \n",
    "        # add the most related original version of snippet in each of three articles in the list\n",
    "        snippet.append(data['body_text'][snippet_index[0]]['text'])\n",
    "        \n",
    "        #csv_path = 'C:/Users/17820/Downloads/metadata.csv'\n",
    "        # prepare the three top articles' paper_ids for output\n",
    "        paper_ids = [processed_data.iloc[index]['paper_id'] for index in top_n_indices]\n",
    "        # prepare the three top articles' similarities for output\n",
    "        similarities = [cos_similarities[index] for index in top_n_indices]\n",
    "    return snippet,paper_ids,similarities \n",
    "    \n",
    "def extract_answer(query, snippet):\n",
    "    \"\"\"\n",
    "    Extract a specific answer from the snippet based on the question word.\n",
    "\n",
    "    query: The input query (question)\n",
    "    snippet: The snippet from the most relevant article\n",
    "    \"\"\"\n",
    "    query_tags = pos_tag(word_tokenize(query))\n",
    "    question_word = \"\"\n",
    "    for word, pos in query_tags:\n",
    "        if pos in [\"WDT\", \"WP\", \"WP$\", \"WRB\"]:\n",
    "            question_word = word.lower()\n",
    "            break\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(snippet)\n",
    "    if question_word in [\"who\", \"what\", \"which\"]:\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in [\"PERSON\", \"ORG\"]:\n",
    "                return ent.text\n",
    "\n",
    "    elif question_word == \"when\":\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in [\"DATE\", \"TIME\"]:\n",
    "                return ent.text\n",
    "\n",
    "    elif question_word == \"where\":\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in [\"GPE\", \"LOC\", \"FAC\"]:\n",
    "                return ent.text\n",
    "\n",
    "    elif question_word in [\"how\", \"why\"]:\n",
    "        for sent in doc.sents:\n",
    "            for token in sent:\n",
    "                if token.dep_ == \"ROOT\":\n",
    "                    return sent.text\n",
    "    \n",
    "    \n",
    "\n",
    "#In the match_articles() function I combined the method of finding the article in the pd and the text in the article, this step is also the most important step of the whole system.\n",
    "#First we extracted the noun, verb and adjective parts of the query, here I did not choose to use NER but use pos_tag because NER can not identify the key words in the question well, \n",
    "#it may be common nouns (face mask) or the most words (larggest), so extract them all can be better to get the semantics.\n",
    "#Then for each keyword in the noun, I searched in the KB whether it has aliases and added to the tokens of the query, which is very helpful to improve the accuracy of the search and increase the weight of keywords.\n",
    "#For similarity comparison I used cos_similarities to embedding the query and each atricle to get the three most matching articles, \n",
    "#and similarly for each paragraph in the atricle I used this method to find the most relevant snippet, snippet and their scores (similarity) as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fb8730b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w4/xhd6ygzn6_n2rl2603vs0m540000gn/T/ipykernel_51425/3548864112.py:8: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "#device_available = torch.cuda.is_available()\n",
    "device_available = False\n",
    "from IPython.core.display import display, HTML\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "if device_available:\n",
    "    model.cuda()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "def get_dataset(csv_path):\n",
    "    corpus = []\n",
    "    csv_df = pd.read_csv(csv_path).dropna(subset=['authors', 'abstract']).drop_duplicates(subset='abstract')\n",
    "    csv_df = csv_df[csv_df['abstract']!='Unknown']\n",
    "    for ix,row in csv_df.iterrows():\n",
    "        if row['abstract'] and not pd.isna(row['abstract']):\n",
    "            temp_dict = dict()\n",
    "            temp_dict['abstract'] = row['abstract']\n",
    "            temp_dict['title'] = row['title']\n",
    "            temp_dict['authors'] = row['authors']\n",
    "            temp_dict['url'] = row['doi']\n",
    "            temp_dict['publish_time'] = row['publish_time']\n",
    "            corpus.append(temp_dict)\n",
    "    return corpus\n",
    "\n",
    "wrapper = textwrap.TextWrapper(width=80) \n",
    "\n",
    "corpus = get_dataset('metadata.csv')\n",
    "def answer_question_dict(question, keyword=None, show_visualization=False):\n",
    "\n",
    "    '''\n",
    "    Takes a `question` string and an `answer_text` string (which contains the\n",
    "    answer), and identifies the words within the `answer_text` that are the\n",
    "    answer. Prints them out.\n",
    "    '''\n",
    "    # select corpus\n",
    "    answer_text = corpus\n",
    "\n",
    "    # Initializing answers list\n",
    "    answers = {}\n",
    "    min_score = 0\n",
    "    counter = 0 # for stopping iterations earlier\n",
    "    \n",
    "    for answer_option in answer_text:\n",
    "        if keyword and keyword not in answer_option['abstract']:\n",
    "            continue\n",
    "\n",
    "      # ======== Tokenize ========\n",
    "      # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "        input_ids = tokenizer.encode(question, answer_option['abstract'],max_length=512)\n",
    "\n",
    "      # Report how long the input sequence is.\n",
    "      #print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
    "\n",
    "      # ======== Set Segment IDs ========\n",
    "      # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "        sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "      # The number of segment A tokens includes the [SEP] token istelf.\n",
    "        num_seg_a = sep_index + 1\n",
    "\n",
    "      # The remainder are segment B.\n",
    "        num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "      # Construct the list of 0s and 1s.\n",
    "        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "    \n",
    "\n",
    "      # There should be a segment_id for every input token.\n",
    "        assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "      # ======== Evaluate ========\n",
    "      # Run our example question through the model.\n",
    "        \n",
    "        input_ids_tensor = torch.tensor([input_ids])\n",
    "        segment_ids_tensor = torch.tensor([segment_ids])\n",
    "        if device_available:\n",
    "            input_ids_tensor = input_ids_tensor.to('cuda:0')\n",
    "            segment_ids_tensor = segment_ids_tensor.to('cuda:0')\n",
    "\n",
    "        start_scores, end_scores = model(input_ids_tensor, # The tokens representing our input text.\n",
    "                                  token_type_ids=segment_ids_tensor) # The segment IDs to differentiate question from answer_text\n",
    "    \n",
    "      # only review answers with score above threshold\n",
    "        score = round(torch.max(start_scores).item(), 3)\n",
    "\n",
    "        if score>min_score and score>0:\n",
    "\n",
    "        # ======== Reconstruct Answer ========\n",
    "        \n",
    "        # Find the tokens with the highest `start` and `end` scores.\n",
    "            answer_start = torch.argmax(start_scores)\n",
    "            answer_end = torch.argmax(end_scores)\n",
    "\n",
    "\n",
    "        # Get the string versions of the input tokens.\n",
    "            tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "        # Start with the first token.\n",
    "            answer = tokens[answer_start]\n",
    "\n",
    "        # Select the remaining answer tokens and join them with whitespace.\n",
    "            for i in range(answer_start + 1, answer_end + 1):\n",
    "\n",
    "                # If it's a subword token, then recombine it with the previous token.\n",
    "                if tokens[i][0:2] == '##':\n",
    "                    answer += tokens[i][2:]\n",
    "\n",
    "                # Otherwise, add a space then the token.\n",
    "                else:\n",
    "                    answer += ' ' + tokens[i]\n",
    "\n",
    "        # ======== Add Answer to best answers list ========\n",
    "\n",
    "            if len(answers)>4:\n",
    "                min_score = min([d for d in answers.keys()])\n",
    "\n",
    "            if len(answers)==10:\n",
    "                answers.pop(min_score)\n",
    "            answers[score] = [answer, score, '<a href=\"https://doi.org/'+str(answer_option['url'])+'\" target=\"_blank\">' + str(answer_option['title']) +'</a>', answer_option['abstract'], answer_option['publish_time']]\n",
    "\n",
    "            visualization_start = max(answer_start-20,0)\n",
    "            visualization_end = min((answer_end+1)+20,len(tokens))\n",
    "            # Variables needed for graphs\n",
    "            s_scores = start_scores.cpu().detach().numpy().flatten()\n",
    "            e_scores = end_scores.cpu().detach().numpy().flatten()\n",
    "\n",
    "            # We'll use the tokens as the x-axis labels. In order to do that, they all need\n",
    "            # to be unique, so we'll add the token index to the end of each one.\n",
    "            token_labels = []\n",
    "            for (i, token) in enumerate(tokens):\n",
    "                token_labels.append('{:} - {:>2}'.format(token, i))\n",
    "            answers[score] = [answer, score, '<a href=\"https://doi.org/'+str(answer_option['url'])+'\" target=\"_blank\">' + str(answer_option['title']) +'</a>', answer_option['abstract'], answer_option['publish_time'], s_scores, e_scores, token_labels, visualization_start, visualization_end]\n",
    "\n",
    "    # Return dataframe with relevant data\n",
    "    df_columns = ['Answer', 'Confidence', 'Title', 'Abstract', 'Published', 's_scores', 'e_scores', 'token_labels', 'visualization_start', 'visualization_end']\n",
    "    df = pd.DataFrame.from_dict(answers, orient='index',columns = df_columns)\n",
    "    df.sort_values(by=['Confidence'], inplace=True, ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eabc5194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_or_run_answer_question_dict(question,keyword):\n",
    "    pickle_name = question.replace(' ','_').replace('?','_')\n",
    "    path_to_file = F\"/kaggle/input/kaggle/{pickle_name}.pickle\"\n",
    "    print(path_to_file)\n",
    "    try:\n",
    "      df = pickle.load(open(path_to_file, \"rb\"))\n",
    "    except (OSError, IOError) as e:\n",
    "        df = answer_question_dict(question, keyword)\n",
    "        pickle.dump(df, open(path_to_file, \"wb\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7ea3a2",
   "metadata": {},
   "source": [
    "### 6. Simple user interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eef48b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [{'question':\"Is smoking a risk factor?\",'keyword':None},\n",
    "             {'question':\"Is a pre-existing pulmonary disease a risk factor?\",'keyword':None},\n",
    "             {'question':\"Do co-existing conditions make the virus more transmissible?\",'keyword':None},\n",
    "             {'question':\"Is being a pregnant woman a risk factor?\",'keyword':'pregnant'},\n",
    "             {'question':\"Is being a neonate a risk factor?\",'keyword':'neonate'},\n",
    "             {'question':\"Are there differences in risk factors associated to socio-economic factors?\",'keyword':None},\n",
    "             {'question':\"How does the transmission happen?\",'keyword':'transmission'},\n",
    "             {'question':\"What is the reproductive rate?\",'keyword':None},\n",
    "             {'question':\"What is the incubation period?\",'keyword':None},\n",
    "             {'question':\"What are the modes of transmission?\",'keyword':None},\n",
    "             {'question':\"What are the enviromental factors?\",'keyword':None},\n",
    "             {'question':\"How long is the serial interval?\",'keyword':None},\n",
    "             {'question':\"What is the severity of disease among high risk groups and patients?\",'keyword':None},\n",
    "             {'question':\"What is the risk of death among high risk groups and patients?\",'keyword':None},\n",
    "             {'question':\"What is the susceptibility of populations?\",'keyword':None},\n",
    "             {'question':\"What are the public health mitigation measures that could be effective for control?\",'keyword':None}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "99cf2087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your question (or 'quit' to exit): \n",
      "Where the first case of COVID-19 was found?\n",
      "answer is China\n",
      "------------------------------------------------------------------------------\n",
      "Enter your question (or 'quit' to exit): \n",
      "When the first case of COVID-19 was found?\n",
      "answer is 26 March 2020\n",
      "------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [87]\u001B[0m, in \u001B[0;36m<cell line: 25>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     23\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m------------------------------------------------------------------------------\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 26\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [87]\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# I built a simple user input interface, so that you can freely select the question input and get the result until you type 'quit'\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m---> 11\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mEnter your question (or \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mquit\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m to exit): \u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m query\u001B[38;5;241m.\u001B[39mstrip()\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mquit\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m     13\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32mE:\\ANACONDA\\lib\\site-packages\\ipykernel\\kernelbase.py:1075\u001B[0m, in \u001B[0;36mKernel.raw_input\u001B[1;34m(self, prompt)\u001B[0m\n\u001B[0;32m   1071\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_allow_stdin:\n\u001B[0;32m   1072\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(\n\u001B[0;32m   1073\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1074\u001B[0m     )\n\u001B[1;32m-> 1075\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_input_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1076\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1077\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_parent_ident\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1078\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_parent\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1079\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1080\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\ANACONDA\\lib\\site-packages\\ipykernel\\kernelbase.py:1120\u001B[0m, in \u001B[0;36mKernel._input_request\u001B[1;34m(self, prompt, ident, parent, password)\u001B[0m\n\u001B[0;32m   1117\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m   1118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[0;32m   1119\u001B[0m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[1;32m-> 1120\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInterrupted by user\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m   1121\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1122\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid Message:\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load data and preprocess\n",
    "    dataset_folder = \"C:/Users/17820/Downloads/pmc_json/\"\n",
    "    processed_data = read_and_process_data(dataset_folder)\n",
    "    #kb = create_kb(processed_data)\n",
    "    kb = {}\n",
    "    model, article_embeddings = create_article_embeddings(processed_data)\n",
    "    \n",
    "    # I built a simple user input interface, so that you can freely select the question input and get the result until you type 'quit'\n",
    "    while True:\n",
    "        query = input(\"Enter your question (or 'quit' to exit): \\n\")\n",
    "        if query.strip().lower() == 'quit':\n",
    "            break\n",
    "\n",
    "        snippet,paper_ids,similarities = match_articles(query, model, processed_data,article_embeddings, kb)\n",
    "        answer = []\n",
    "        for s in snippet:\n",
    "            ans = extract_answer(query,s)\n",
    "            answer.append(ans)\n",
    "        answer = Counter(answer)\n",
    "        answer = list(answer.keys())[0]\n",
    "        print(f\"answer is {answer}\")\n",
    "        print('------------------------------------------------------------------------------')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "# For my custom questions, the answer system gave some more relevant answers, in terms of improving performance, \n",
    "#using larger data, more accurate data cleaning and using pre-trained models are expected to improve the accuracy and score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a91b4",
   "metadata": {},
   "source": [
    "### 7. References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TREC-COVID Qrels 1, Retrieved from https://ir.nist.gov/covidSubmit/data.html#cumu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "372b784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "try:\n",
    "    import transformers\n",
    "    from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "except ImportError:\n",
    "    raise ImportError(INSTALL_MSG)\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "LONG_BORING_TENNIS_ARTICLE = \"\"\"\n",
    " Andy Murray  came close to giving himself some extra preparation time for his w\n",
    "edding next week before ensuring that he still has unfinished tennis business to\n",
    " attend to. The world No 4 is into the semi-finals of the Miami Open, but not be\n",
    "fore getting a scare from 21 year-old Austrian Dominic Thiem, who pushed him to \n",
    "4-4 in the second set before going down 3-6 6-4, 6-1 in an hour and three quarte\n",
    "rs. Murray was awaiting the winner from the last eight match between Tomas Berdy\n",
    "ch and Argentina's Juan Monaco. Prior to this tournament Thiem lost in the secon\n",
    "d round of a Challenger event to soon-to-be new Brit Aljaz Bedene. Andy Murray p\n",
    "umps his first after defeating Dominic Thiem to reach the Miami Open semi finals\n",
    " . Muray throws his sweatband into the crowd after completing a 3-6, 6-4, 6-1 vi\n",
    "ctory in Florida . Murray shakes hands with Thiem who he described as a 'strong \n",
    "guy' after the game . And Murray has a fairly simple message for any of his fell\n",
    "ow British tennis players who might be agitated about his imminent arrival into \n",
    "the home ranks: don't complain. Instead the British No 1 believes his colleagues\n",
    " should use the assimilation of the world number 83, originally from Slovenia, a\n",
    "s motivation to better themselves. At present any grumbles are happening in priv\n",
    "ate, and Bedene's present ineligibility for the Davis Cup team has made it less \n",
    "of an issue, although that could change if his appeal to play is allowed by the \n",
    "International Tennis Federation. Murray thinks anyone questioning the move, now \n",
    "it has become official, would be better working on getting their ranking closer \n",
    "to his. 'If he was 500 in the world they wouldn't be that fussed about it but ob\n",
    "viously he threatens their position a bit,' said the 27 year-old Scot. ' and he'\n",
    "s obviously the British number two, comfortably. 'So they can complain but the b\n",
    "est thing to do is use it in the right way and accept it for what it is, and try\n",
    " to use it as motivation whether they agree with it or not. He's British now so \n",
    "they've just got to deal with it. Murray stretches for a return after starting h\n",
    "is quarter final match slowly on the show court . Thiem held nothing back as he \n",
    "raced through the opening set, winning it 6-3 with a single break . The young Au\n",
    "strian is considered to be one of the hottest prospects on the ATP Tour . 'I wou\n",
    "ld hope that all the guys who are below him now like James (Ward) , Kyle (Edmund\n",
    ") , Liam (Broady) they will use it as motivation. If he becomes eligible for Dav\n",
    "is Cup then those guys are going to have to prove themselves. 'It can only be se\n",
    "en as a positive for those guys using it to try to get better. He's a good playe\n",
    "r but so are James and Kyle and Liam has improved. Aljaz is there, he's on the t\n",
    "our every week, the other guys aren't quite there yet.' For the first time Murra\n",
    "y, who has an encyclopaedic knowledge of the top 100, gave his opinion of Bedene\n",
    ": 'He's a good player with a very good serve. He's a legitimate top 100 player, \n",
    "when he plays Challengers he's there or thereabouts, when he plays on the main t\n",
    "our he wins matches, it's not like he turns up and always loses in the first rou\n",
    "nd. Murray's fiancee was once again watching from the stands shaded by a huge br\n",
    "immed hat . Kim Sears flashes her enormous diamond engagement ring while watchin\n",
    "g her beau on court . 'He had a bad injury last year (wrist) but has recovered w\n",
    "ell. I would imagine he would keep moving up the rankings although I don't know \n",
    "exactly how high he can go. I've practised with him a couple of times, I haven't\n",
    " seen him play loads, but when you serve as well as he does it helps. I would im\n",
    "agine he' s going to be comfortably in the top 70 or 80 in the world for a while\n",
    ".' It is understood the Lawn Tennis Association will give background support to \n",
    "his case regarding the Davis Cup but have made it clear that the onus is on him \n",
    "to lead the way. An official statement said: 'To have another player in the men'\n",
    "s top 100 is clearly a positive thing for British tennis and so we very much wel\n",
    "come Aljaz's change in citizenship.' The last comparable switch came twenty year\n",
    "s ago when Greg Rusedski arrived from Canada. It was by no means universally pop\n",
    "ular but, like Bedene, he pledged that he was in for the long haul and, in fairn\n",
    "ess to him, he proved true to his word. Loising the first set shocked Murray int\n",
    "o life as he raced to a commanding lead in the second . The No 3 seed sent over \n",
    "a few glaring looks towards his team before winning the second set . Murray had \n",
    "to put such matters aside as he tackled the unusually talented Thiem, a delight \n",
    "to watch. Coached by Boris Becker's veteran mentor Gunter Bresnik, he slightly r\n",
    "esembles Andy Roddick and hits with similar power but more elegance. His single \n",
    "handed backhand is a thing of rare beauty. However, he has had a mediocre season\n",
    " coming into this event and there was little to forewarn of his glorious shotmak\n",
    "ing that seemed to catch Murray unawares early on. The world No 4 looked to have\n",
    " worked him out in the second, but then suffered one of his periopdic mental lap\n",
    "ses and let him back in from 4-1 before closing it out with a break. After break\n",
    "ing him for 3-1 in the decider the Austrian whirlwind burnt itself out. 'He's a \n",
    "strong guy who hits the ball hard and it became a very physical match,' said Mur\n",
    "ray. Murray was presented with a celebratory cake after winning his 500th match \n",
    "in the previous round .\n",
    "\"\"\".replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b121c260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 20:08:50.803729: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c981d41619e4c9382f8393a66d14701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1e1e67d18f465f8992c9a40badb00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4b91e2fb564a1886ab73b86d3c29e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c27f4bbe4fc437eaea70f675cf43e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41e6a4f9cd9476aac2e859e182f4a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3de2fd458d24fa4a32add7c43fc034f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "\"\"\"\n",
    "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))\n",
    "[{'summary_text': 'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6fb6135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Covid started in china'}]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Covid started in china, covid started in wuhan, covid in china, covid started in 2019, china might be origin of covid\"\n",
    "print(summarizer(sentence, max_length=10, min_length=10, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99aa63ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'In the pandemic context, infants younger than 3 months with isolated fever should be tested for SARS-CoV-2.'}]\n"
     ]
    }
   ],
   "source": [
    "art = \"\"\"It is anticipated that COVID-19 will spread further outside of Tokyo, \n",
    "therefore, the preparation for the COVID-19 emergency described above will be useful for \n",
    "those regions where the number of infected patients is still low.', Here we describe our \n",
    "experience of COVID-19 in five young infants. In the pandemic context, infants younger than \n",
    "3 months with isolated fever should be tested for SARS-CoV-2. Although infants might initially \n",
    "present signs of severe infection, our experience is that the youngest children tolerate and \n",
    "rapidly improve from COVID-19, in contrast to adults admitted to hospital with COVID-19. \n",
    "However, because little is known about SARS-CoV-2 infection in infants,4, 6 close monitoring \n",
    "is required for at least 2 weeks after the diagnosis. All of the infants' parents showed mild \n",
    "signs of viral infection (ie, rhinitis, or cough or fever, or both, for <1 week), which could\n",
    "be related to undiagnosed COVID-19.\", 'Different viral agents are associated with an increased \n",
    "risk of more severe disease course and respiratory complications in immunocompromised patients.\n",
    "1–3 The recent outbreak of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) \n",
    "disease 2019 (COVID-19) responsible for a severe acute respiratory syndrome (SARS) represents \n",
    "a source of concern for the management of patients with inflammatory rheumatic diseases. \n",
    "Lombardy is the region in Northern Italy with the highest incidence of COVID-19 cases, \n",
    "with more than 33 000 confirmed patients and 1250 requiring admission to the intensive \n",
    "care unit within 1 month. Since the first reports of COVID-19 cases in Italy, we have circulated \n",
    "a survey with a 2-week follow-up contact to patients with chronic arthritis treated with \n",
    "biological disease-modifying antirheumatic drugs (bDMARDs) or targeted synthetic \n",
    "disease-modifying antirheumatic drugs (tsDMARDs) followed up at our biological outpatient \n",
    "clinic in Pavia, Lombardy. The survey investigated the patients’ health conditions, the presence \n",
    "of contacts with subjects known to be affected by COVID-19 and management of the DMARDs \n",
    "during the first few weeks of pandemic. All patients had provided their informed consent\n",
    "for the use of personal and clinical data for scientific purposes, and no patient refused \n",
    "to participate.\"\"\"\n",
    "print(summarizer(art, max_length=30, min_length=10, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b569b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' clinical samples from all patients including nasopharyngeal swab (nps)'}]\n"
     ]
    }
   ],
   "source": [
    "question = \"Can the virus be found in nasal discharge, sputum, urine, fecal matter, or blood\"\n",
    "sentence = \"\"\"clinical samples from all patients including nasopharyngeal swab (nps) / sputum, urine, and feces were collected for serial virus rna testing by standard qrt-pcr assay.\n",
    "methods matched sputum samples, ops, blood cultures, serum, and urine samples were taken from patients (> 18 years) with cap and tested for the presence of possible respiratory pathogens using bacterial cultures, pcr for 17 viruses and five bacteria and urinary antigen testing.\n",
    "background : the world health organization alert for the h1n1 influenza pandemic led to the implementation of certain measures regarding admission of patients with flu-like symptoms.\n",
    "\"\"\"\n",
    "\n",
    "print(summarizer(sentence, max_length=20, min_length=10, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbce2112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Middle east respiratory syndrome (mers-cov) can cause severe and fatal acute respiratory disease in humans. There are currently no approved vaccines or therapies available for mers-Cov.'}]\n"
     ]
    }
   ],
   "source": [
    "test=\"\"\"was 2. 81 % has lower severity and mortality than sars but is much more transmissive\n",
    "and affects more elderly individuals than youth and more men than women sars-cov-2 is highly\n",
    "contagious and often leads to severe viral pneumonia with respiratory failure and death in the\n",
    "elderly and subjects with pre-existing conditions sero prevalence of sars-cov-2 antibodies may \n",
    "help in more accurate estimations of the total number of cases adults represent the population \n",
    "with the highest infection rate at 4 % prevalence we save 74 % at 1 % we save 91 % and at \n",
    "1 % we save 97 % of tests. we analyze the number of times each sample is used and show the \n",
    "method is still efficient if we resort to testing a case individually if the sample is running\n",
    "low. abstract in addition we recommend clinical screening to filter out individuals with symptoms\n",
    "and show this leaves us with a population with lower prevalence the true number of people currently\n",
    "infected with the virus, divided by the total population size.middle east respiratory syndrome\n",
    "coronavirus (mers-cov) can cause severe and fatal acute respiratory disease in humans \n",
    "and remains endemic in the middle east since first being identified in 2012. there \n",
    "are currently no approved vaccines or therapies available for mers-cov. in this study, we \n",
    "evaluated parainfluenza virus 5 (piv5)-based vaccine expressing the mers-cov envelope spike \n",
    "protein (piv5 / mers-s) in a human dpp4 knockin c57bl / 6 congenic mouse model (hdpp4 ki). \n",
    "following a single-dose intranasal immunization, piv5-mers-s induced neutralizing antibody \n",
    "and robust t cell responses in hdpp4 ki mice. a single intranasal administration of 10 (4) \n",
    "pfu piv5-mers-s provided complete protection against a lethal challenge with mouse-adapted \n",
    "mers-cov (mersma6. 1. 2) and improved virus clearance in the lung. in comparison, single-dose \n",
    "intramuscular immunization with 10 (6) pfu uv-inactivated mersma6. 1. 2 mixed with imject\n",
    "alum provided protection to only 25 % of immunized mice. intriguingly, an influx of eosinophils\n",
    "was observed only in the lungs of mice immunized with inactivated mers-cov, suggestive of \n",
    "a hypersensitivity-type response. overall, our study indicated that piv5-mers-s is a promising\n",
    "effective vaccine candidate against mers-cov infection. importance mers-cov causes lethal\n",
    "infection in humans, and there is no vaccine. our work demonstrates that piv5 is a promising\n",
    "vector for developing a mers vaccine. furthermore, success of piv5-based mers vaccine can\n",
    "be employed to develop a vaccine for emerging covs such as sars-cov-2, which causes covid-19\"\"\"\n",
    "print(summarizer(test, max_length=40, min_length=10, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62dfa5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
